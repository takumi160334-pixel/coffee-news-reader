import os
from google import genai
from google.genai import types
from typing import Dict, Any, List
from pydantic import BaseModel, Field
import time
import config

# --- Pydantic Schemas for Structured Output ---

class ProcessedArticle(BaseModel):
    index: int = Field(description="The index ID of the original article this refers to.")
    category_id: int = Field(description="The category ID (1-7) that best fits the article. 1 if none fit perfectly.")
    summary: str = Field(description="A concise 2-3 line Japanese summary of the article. Must not contain PII.")

class BatchResult(BaseModel):
    articles: List[ProcessedArticle] = Field(description="List of processed results for the given articles.")

# --- End Schemas ---

class NewsProcessor:
    def __init__(self, api_key: str):
        """Initialize the Gemini client."""
        self.client = genai.Client(api_key=api_key)
        self.model = 'gemini-2.5-flash-lite' 

    def process_articles_in_chunks(self, articles: List[Dict[str, Any]], chunk_size: int = 20) -> List[Dict[str, Any]]:
        """Process a list of articles by chunking them to avoid token limits & hallucinations."""
        processed_articles = []
        
        # Split articles into chunks
        chunks = [articles[i:i + chunk_size] for i in range(0, len(articles), chunk_size)]
        total_chunks = len(chunks)
        
        print(f"ğŸ“¦ è¨˜äº‹ã‚’ {total_chunks} å€‹ã®ãƒãƒƒãƒï¼ˆãƒãƒ£ãƒ³ã‚¯ï¼‰ã«åˆ†å‰²ã—ã¦ä¸€æ‹¬å‡¦ç†ã—ã¾ã™...")
        
        for i, chunk in enumerate(chunks, 1):
            print(f"  â³ ãƒãƒƒãƒ {i}/{total_chunks} ã‚’å‡¦ç†ä¸­ ({len(chunk)} ä»¶)...")
            processed_chunk = self._process_batch_two_pass(chunk)
            processed_articles.extend(processed_chunk)
            
            # API Quota Rate Limiting (15 RPM for free tier)
            if i < total_chunks:
                time.sleep(10) # Safe delay between batches
                
        return processed_articles

    def _process_batch_two_pass(self, chunk: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """The two-pass architecture: 1. Generate Summaries, 2. Double-Check."""
        
        # 1. Prepare the input payload
        input_payload = ""
        for i, article in enumerate(chunk):
            title = article.get('title', '')
            content = article.get('content', '')
            # Truncate very long articles to respect context window and attention
            if len(content) > 1500:
                content = content[:1500] + "...(truncated)"
            
            input_payload += f"--- ARTICLE INDEX: {i} ---\nTITLE: {title}\nCONTENT: {content}\n\n"

        # --------------------------------------------------------------------
        # PASS 1: Initial Translation and Categorization
        # --------------------------------------------------------------------
        pass1_prompt = f"""
        ã‚ãªãŸã¯å„ªç§€ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ç·¨é›†è€…ã§ã™ã€‚ä»¥ä¸‹ã®è¤‡æ•°è¨˜äº‹ã‚’ä¸€æ‹¬ã§å‡¦ç†ã—ã¦ãã ã•ã„ã€‚
        
        ã€å³å®ˆäº‹é …ã€‘
        1. å„è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªç•ªå·ï¼ˆ1ã€œ7ï¼‰ã«åˆ†é¡ã—ã€2ã€œ3è¡Œã®æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        2. å€‹äººæƒ…å ±ä¿è­·: äººåã€ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ã€é›»è©±ç•ªå·ãªã©ã®å€‹äººæƒ…å ±(PII)ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯(***)ã§ãƒã‚¹ã‚¯ã—ã¦çµ¶å¯¾ã«è¦ç´„ã«å‡ºåŠ›ã—ãªã„ã§ãã ã•ã„ã€‚
        3. å¿…é ˆ: å‡ºåŠ›ã™ã‚‹å„JSONãƒ‡ãƒ¼ã‚¿ã® `index` ã«ã¯ã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ã€ŒARTICLE INDEXã€ã®æ•°å€¤ã‚’å¿…ãšãã®ã¾ã¾è¨­å®šã—ã¦ãã ã•ã„ã€‚
        
        ã€è¨˜äº‹ãƒ‡ãƒ¼ã‚¿ã€‘
        {input_payload}
        """

        print("     -> [Pass 1] åˆæœŸç¿»è¨³ã¨è¦ç´„ã‚’å®Ÿè¡Œä¸­...")
        pass1_result = self._call_gemini_structured(pass1_prompt, "Pass 1")
        
        if not pass1_result:
            return self._build_fallback_chunk(chunk)

        # --------------------------------------------------------------------
        # PASS 2: Double-Check & Hallucination Prevention
        # --------------------------------------------------------------------
        pass2_prompt = f"""
        ã‚ãªãŸã¯éå¸¸ã«å³æ ¼ãªã€Œç›£æŸ»å½¹ï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚«ãƒ¼ï¼‰ã€ã§ã™ã€‚
        ã€å…ƒãƒ‡ãƒ¼ã‚¿ã€‘ã¨ã€AIãŒä¸€æ™‚çš„ã«ä½œæˆã—ãŸã€1å›ç›®ã®å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã€‘ã‚’æ¯”è¼ƒã—ã€é–“é•ã„ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
        
        ã€ç›£æŸ»ã®åŸºæº–ã€‘
        1. åŸæ–‡ã«ãªã„äº‹å®Ÿï¼ˆãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ï¼‰ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ï¼Ÿã‚ã‚Œã°å‰Šé™¤ãƒ»ä¿®æ­£ã€‚
        2. å€‹äººæƒ…å ±(PII)ãŒæ¼ã‚Œã¦ã„ãªã„ã‹ï¼Ÿã‚ã‚Œã°ãƒã‚¹ã‚¯(***)ã™ã‚‹ã€‚
        3. ç¿»è¨³ã®ç²¾åº¦ã¯é©åˆ‡ã‹ï¼Ÿ
        4. è¦ç´„ã¯å†—é•·ã«ãªã£ã¦ã„ãªã„ã‹ï¼Ÿ
        5. å¿…é ˆ: æœ€çµ‚çš„ãªJSONãƒ‡ãƒ¼ã‚¿ã® `index` ã«ã¯ã€å¿…ãšã€å…ƒãƒ‡ãƒ¼ã‚¿ã€‘ã®ã€ŒARTICLE INDEXã€ã¨ä¸€è‡´ã™ã‚‹æ•°å€¤ã‚’è¨­å®šã™ã‚‹ã“ã¨ã€‚
        
        ã“ã‚Œã‚‰ã®åŸºæº–ã§ã™ã¹ã¦ã®è¦ç´„ã‚’å¯©æŸ»ã—ã€å®Œç’§ãªæœ€çµ‚ç‰ˆã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
        
        ã€å…ƒãƒ‡ãƒ¼ã‚¿ã€‘
        {input_payload}
        
        ã€1å›ç›®ã®å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿ã€‘
        {pass1_result.model_dump_json()}
        """
        
        print("     -> [Pass 2] äºŒé‡ç›£æŸ»ï¼ˆãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ï¼‰ã‚’å®Ÿè¡Œä¸­...")
        pass2_result = self._call_gemini_structured(pass2_prompt, "Pass 2")
        
        final_result = pass2_result if pass2_result else pass1_result
        
        if final_result:
            processed_chunk, failed_originals = self._merge_results(chunk, final_result)
            
            # --- AUTO-HEALING RECOVERY SYSTEM ---
            if failed_originals:
                print(f"     âš ï¸ {len(failed_originals)}ä»¶ã®è¨˜äº‹ã®è¦ç´„å‡ºåŠ›ãŒæ¬ æã—ã¦ã„ã¾ã—ãŸã€‚è‡ªå‹•å¾©æ—§ï¼ˆè‡ªå·±ä¿®å¾©ãƒªã‚«ãƒãƒªãƒ¼ï¼‰ã‚’é–‹å§‹ã—ã¾ã™...")
                recovered_chunk = self._recover_failed_articles(failed_originals)
                processed_chunk.extend(recovered_chunk)
                
            return processed_chunk
        else:
            return self._build_fallback_chunk(chunk)

    def _call_gemini_structured(self, prompt: str, stage_name: str) -> BatchResult | None:
        """Call Gemini API utilizing Structured Outputs to ensure perfect JSON matching."""
        max_retries = 3
        base_delay = 15

        for attempt in range(max_retries):
            try:
                response = self.client.models.generate_content(
                    model=self.model,
                    contents=prompt,
                    config=types.GenerateContentConfig(
                        system_instruction=config.GEMINI_SYSTEM_PROMPT,
                        temperature=0.1, # Extremely low for deterministic fact-checking
                        response_mime_type="application/json",
                        response_schema=BatchResult,
                    )
                )
                
                # The response.parsed is an instance of the Pydantic schema
                if hasattr(response, 'parsed') and response.parsed:
                    return response.parsed
                else:
                    raise ValueError("Gemini API returned a successful response, but 'parsed' was missing or empty.")
                
            except Exception as e:
                error_msg = str(e)
                print(f"       âš ï¸ {stage_name} Error (Attempt {attempt + 1}/{max_retries}): {error_msg}")
                if attempt < max_retries - 1:
                    sleep_time = 65 if "429" in error_msg else base_delay * (attempt + 1)
                    time.sleep(sleep_time)
                else:
                    print(f"       âŒ {stage_name} completely failed after retries.")
                    return None

    def _merge_results(self, original_chunk: List[Dict[str, Any]], batch_result: BatchResult) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Merge the validated JSON summaries back into the original article dictionaries."""
        
        # Create a lookup dictionary from the structured output
        result_lookup = {item.index: item for item in batch_result.articles}
        
        processed_chunk = []
        failed_originals = []
        
        for i, article in enumerate(original_chunk):
            processed_article = dict(article) # Copy
            
            # Find the corresponding processed data by ID
            if i in result_lookup:
                ai_data = result_lookup[i]
                
                # Category logic (1-based to 0-based index)
                cat_idx = ai_data.category_id - 1
                if 0 <= cat_idx < len(config.CATEGORIES):
                    processed_article['category'] = config.CATEGORIES[cat_idx]
                else:
                    processed_article['category'] = config.CATEGORIES[0]
                    
                processed_article['summary'] = ai_data.summary
                processed_chunk.append(processed_article)
            else:
                 # The AI skipped this article somehow (Hallucination loss)
                 failed_originals.append(article)
            
        return processed_chunk, failed_originals

    def _build_fallback_chunk(self, chunk: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Creates dummy data if API catastrophically fails."""
        processed_chunk = []
        for article in chunk:
            processed_article = dict(article)
            processed_article['category'] = config.CATEGORIES[0]
            processed_article['summary'] = "APIåˆ¶é™ãªã©ã«ã‚ˆã‚Šè‡ªå‹•è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            processed_chunk.append(processed_article)
        return processed_chunk

    def _recover_failed_articles(self, failed_chunk: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """A dedicated auto-healing process for articles completely dropped by Gemini during batching."""
        input_payload = ""
        for i, article in enumerate(failed_chunk):
            title = article.get('title', '')
            content = article.get('content', '')[:1500]
            input_payload += f"--- ARTICLE INDEX: {i} ---\nTITLE: {title}\nCONTENT: {content}\n\n"

        recovery_prompt = f"""
        ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„ä¿®å¾©AIã§ã™ã€‚å‰å›ã®å‡¦ç†ã§AIã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šæ¬ è½ã—ãŸè¨˜äº‹ã®æ•‘æ¸ˆã‚’è¡Œã„ã¾ã™ã€‚
        
        ã€å³å®ˆäº‹é …ã€‘
        1. å„è¨˜äº‹ã‚’ã‚«ãƒ†ã‚´ãƒªç•ªå·ï¼ˆ1ã€œ7ï¼‰ã«åˆ†é¡ã—ã€2ã€œ3è¡Œã®æ—¥æœ¬èªã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚
        2. å€‹äººæƒ…å ±(PII)ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯(***)ã§ãƒã‚¹ã‚¯ã—ã¦ãã ã•ã„ã€‚
        3. å¿…é ˆ: å‡ºåŠ›ã™ã‚‹å„JSONãƒ‡ãƒ¼ã‚¿ã® `index` ã«ã¯ã€å¿…ãšã€ŒARTICLE INDEXã€ã®æ•°å€¤ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚
        
        ã€æ•‘æ¸ˆå¯¾è±¡ãƒ‡ãƒ¼ã‚¿ã€‘
        {input_payload}
        """

        time.sleep(10) # Delay to respect RPM before rapid recovery
        recovery_result = self._call_gemini_structured(recovery_prompt, "Recovery Pass")
        
        if not recovery_result:
            return self._build_fallback_chunk(failed_chunk)
            
        result_lookup = {item.index: item for item in recovery_result.articles}
        recovered_chunk = []
        
        for i, article in enumerate(failed_chunk):
            processed_article = dict(article)
            if i in result_lookup:
                ai_data = result_lookup[i]
                cat_idx = ai_data.category_id - 1
                processed_article['category'] = config.CATEGORIES[cat_idx] if 0 <= cat_idx < len(config.CATEGORIES) else config.CATEGORIES[0]
                processed_article['summary'] = ai_data.summary
            else:
                # Absolute catastrophic failure (failed even on recovery)
                processed_article['category'] = config.CATEGORIES[0]
                processed_article['summary'] = "è‡ªå‹•å¾©æ—§å‡¦ç†ï¼ˆè‡ªå·±ä¿®å¾©ãƒªã‚«ãƒãƒªãƒ¼ï¼‰ã§ã‚‚è¦ç´„ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"
            
            recovered_chunk.append(processed_article)
            
        return recovered_chunk
